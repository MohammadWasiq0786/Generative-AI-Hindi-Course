{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b6524c2-5754-4a45-a303-505426e03d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111186a-4244-4c68-9389-24ffd8c523e9",
   "metadata": {},
   "source": [
    "## Previous name: GPT Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5f3ba-5209-4706-8040-511a9b959b4f",
   "metadata": {},
   "source": [
    "## Home Page: Pitch\n",
    "* Unleash the power of LLMs over your data\n",
    "    * Data Ingestion\n",
    "        * Unstructured data: PDF, Text, Video, Images, etc.\n",
    "        * Strucured data: Excel, SQL, etc.\n",
    "        * Semi-strucured data: API's Slack, Salesforce, Notion, etc. \n",
    "    * Data Indexing\n",
    "        * Store (save)\n",
    "        * Index (find)\n",
    "        * Integrate with vector stores and databases \n",
    "    * Query Interface\n",
    "        * Accepts any input prompt over your data\n",
    "        * Returns a knowledge-augmented response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c4da6-602c-4cec-8e19-3d0c4d2aa8ac",
   "metadata": {},
   "source": [
    "## Home Page: Use Cases\n",
    "* Document QA\n",
    "* Data Augmented Chatbots\n",
    "* Knowledge Agents\n",
    "* Structured Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbecb3a-dd36-49de-ac3a-f554073267b8",
   "metadata": {},
   "source": [
    "## Home Page: Products\n",
    "* LlamaIndex (Python)\n",
    "* LlamaIndex.TS (Typescript version)\n",
    "* LlamaHub\n",
    "    * Llama Packs\n",
    "    * Data Loaders\n",
    "    * Agent tools \n",
    "* SEC Insights: end to end app\n",
    "* create-llama: CLI tool to install llamaindex from terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b808ecc-23e7-4e21-9343-2235ce983e93",
   "metadata": {},
   "source": [
    "## Last features\n",
    "* [RAGs](https://github.com/run-llama/rags):\n",
    "    * Build, customize, and use multiple ChatGPTs over your data, all with natural language.\n",
    "    * RAGs is a Streamlit app that lets you create a RAG pipeline from a data source using natural language.\n",
    "* [LLama Packs](https://llamahub.ai/). Interesting llama packs:\n",
    "    * Resume screener\n",
    "    * Gmail OpenAI agent\n",
    "    * Deeplake multimodal retrieval\n",
    "    * Sub_question Webiate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9cfbc1-e3b0-4700-9fc2-354431e2ae6c",
   "metadata": {},
   "source": [
    "## Documentation: structure\n",
    "* Getting started\n",
    "* Use cases\n",
    "* Understanding LLamaIndex\n",
    "    * Tutorial series \n",
    "* Optimizing\n",
    "    * When you already have LlamaIndex app working and are looking to further refine it.\n",
    "    * List of first things you should try: embedding model, chunk size, customizations, etc.\n",
    "    * Fine tuning your model.\n",
    "* Module guides\n",
    "    * Guides to the individual components of LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbca7f5-4787-4e01-8604-17d25aae919c",
   "metadata": {},
   "source": [
    "## Documentation: Starter Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298937ea-5594-4834-8df9-9bb41e37ceec",
   "metadata": {},
   "source": [
    "#### Load Private Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b7147-5e15-4093-86b7-355fcc4a56b6",
   "metadata": {},
   "source": [
    "#### Create Vector Database (LlamaIndex call them \"indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0303edf-ddd3-4219-b5b8-2e93cdbd90be",
   "metadata": {},
   "source": [
    "#### QA over private document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151655aa-87a8-4eb4-92c6-9077ad7b41e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "684447bd-69be-45a2-b159-12626f795092",
   "metadata": {},
   "source": [
    "#### Save the vector database in your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c4c3251-2b19-4a1c-b7ba-38118528ef80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f412dd7-72a1-4cc0-b216-80cab6e027d7",
   "metadata": {},
   "source": [
    "By default, this will save the data to the directory storage, but you can change that by passing a `persist_dir` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c3c57-4401-4efb-b90b-6297ded5cd57",
   "metadata": {},
   "source": [
    "## Documentation: High-Level Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0610bbbe-b468-4a58-a61b-13c1ca733137",
   "metadata": {},
   "source": [
    "#### RAG\n",
    "* Your data is loaded\n",
    "* Your data is indexed: prepared for queries\n",
    "* When you ask a question, LlamaIndex gets the most relevant data from the vector database and passes your question and this most relevant data (called \"the context\") to the LLM so the LLM can redact a conversational answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d72c4d-89fc-40bd-a0a8-61974c742250",
   "metadata": {},
   "source": [
    "#### Stages within RAG\n",
    "1. Loading\n",
    "2. Indexing: convert data into embeddings and metadata\n",
    "3. Storing: store your embeddings and metadata\n",
    "4. Querying\n",
    "    * sub-queries\n",
    "    * multi-step queries\n",
    "    * hybrid strategies\n",
    "5. Evaluation: checking how your accurate, faithful and fast responses to queries are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e0a81-5652-462c-8fb4-687f939d445f",
   "metadata": {},
   "source": [
    "#### Important concepts within some of the previous stages\n",
    "1. Loading\n",
    "    * Document: data format (PDF, API, etc).\n",
    "    * Node: data chunk with metadata.\n",
    "    * Connector or Reader: connects with data sources.\n",
    "2. Indexing\n",
    "    * Indexing: transformation and storage of data into embeddings with metadata in vector databases.\n",
    "    * Embeddings: numerical representation of data.\n",
    "4. Querying\n",
    "    * Retrievers: how to retrieve relevant context from an index when given a query. The retrieval strategy is key to the performance of the app.\n",
    "    * Routers: determines which retriever will be used based on the reriever's metadata and the query.\n",
    "    * Node postprocessors: applies transformations, filtering and re-ranking logic to nodes.\n",
    "    * Response synthesizers: given a query and a set of retrieved text chunks, it generates the conversational response from an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9cee4-0280-4100-b71a-de373b962896",
   "metadata": {},
   "source": [
    "#### Naming of the 3 main use cases\n",
    "* Query Engines: ask questions about your data.\n",
    "* Chat Engines: have a conversation with your data.\n",
    "* Agents: automated decision maker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90d6fd-14ab-43aa-ad0d-be89ab7d0e35",
   "metadata": {},
   "source": [
    "## Documentation: Customization Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973baba1-3ab6-4363-b382-e36e4398119b",
   "metadata": {},
   "source": [
    "#### Starting point: basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "adfca0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b5eab-4ae7-4cae-b1f9-cf3ff2e470df",
   "metadata": {},
   "source": [
    "#### Parse the document into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dcff1d04-54c4-41b8-858d-21d1db0b35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llma_index  import ServiceContext\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(chunk_size=1000)\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "befbda15-54aa-40ba-9329-1c6f0a6f683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, \n",
    "#     service_context=service_context\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544dbbb-365a-44ba-8ed3-1866331d7ef8",
   "metadata": {},
   "source": [
    "#### Use a different vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b005c4e7-2ab3-4ee6-a755-d241a7906038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# from llama_index.vector_stores import ChromaVectorStore\n",
    "# from llama_index import StorageContext\n",
    "\n",
    "# chroma_client = chromadb.PersistentClient()\n",
    "# chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "# vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     vector_store=vector_store\n",
    "# )\n",
    "\n",
    "\n",
    "# loading in chromadb \n",
    "import chromadb\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db1\")\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "  documents, storage_context=storage_context, embed_model=Settings.embed_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2dd79c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cac054f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The article discusses the importance of creating something that people want and not focusing solely on making money, highlighting the success of companies like Craigslist that prioritize user satisfaction over maximizing profits. It also shares a story about a startup, Octopart, facing challenges from a larger distributor due to their ethical approach to providing a valuable service. The article emphasizes the benefits of being benevolent in business, attracting talented individuals and support from various stakeholders by prioritizing doing good over profit.', source_nodes=[NodeWithScore(node=TextNode(id_='b97fc4ec-a34c-4d6f-86be-e3ec5c25a94f', embedding=None, metadata={'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6897b2f1-b5f2-4cc5-9b78-a88678788fa0', node_type='4', metadata={'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}, hash='1ab4e23c02c16bcc622b684adcc1e5b09a74d34c7f43f8f9b1e784ecfc419ccf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7320365a-cdf8-49c3-b7b0-d04ae1abf00e', node_type='1', metadata={}, hash='136b5632fb1d23d7c4388dc4156cf04faabc907a88b2d151b1f49fdb2de32f6a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Be good\\n\\nApril 2008(Tl.)About a month after we started Y Combinator we came up with the\\nphrase that became our motto: Make something people want.  We've\\nlearned a lot since then, but if I were choosing now that's still\\nthe one I'd pick.Another thing we tell founders is not to worry too much about the\\nbusiness model, at least at first.  Not because making money is\\nunimportant, but because it's so much easier than building something\\ngreat.A couple weeks ago I realized that if you put those two ideas\\ntogether, you get something surprising.  Make something people want.\\nDon't worry too much about making money.  What you've got is a\\ndescription of a charity.When you get an unexpected result like this, it could either be a\\nbug or a new discovery.  Either businesses aren't supposed to be\\nlike charities, and we've proven by reductio ad absurdum that one\\nor both of the principles we began with is false.  Or we have a new\\nidea.I suspect it's thehis essay is derived from a talk at the 2008 Startup Schoo latter, because as soon as this thought occurred\\nto me, a whole bunch of other things fell into place.ExamplesFor example, Craigslist.  It's not a charity, but they run it like\\none.  And they're astoundingly successful.  When you scan down the\\nlist of most popular web sites, the number of employees at Craigslist\\nlooks like a misprint. Their revenues aren't as high as they could\\nbe, but most startups would be happy to trade places with them.In Patrick O'Brian's novels, his captains always try to get upwind\\nof their opponents.  If you're upwind, you decide when and if to\\nengage the other ship.  Craigslist is effectively upwind of enormous\\nrevenues.  They'd face some challenges if they wanted to make more,\\nbut not the sort you face when you're tacking upwind, trying to\\nforce a crappy product on ambivalent users by spending ten times\\nas much on sales as on development.  [1]I'm not saying startups should aim to end up like Craigslist.\\nThey're a product of unusual circumstances.\", mimetype='text/plain', start_char_idx=0, end_char_idx=1994, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.27772860393972165), NodeWithScore(node=TextNode(id_='8aea0f59-eece-4b2d-9b61-3dc04cc9e3da', embedding=None, metadata={'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6897b2f1-b5f2-4cc5-9b78-a88678788fa0', node_type='4', metadata={'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}, hash='1ab4e23c02c16bcc622b684adcc1e5b09a74d34c7f43f8f9b1e784ecfc419ccf'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a6e917f7-f68b-4183-8515-2713ea49671e', node_type='1', metadata={'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}, hash='b32e65cd31c9439890b1d4d785f87b2b07b30e9f137cf7a9edd7b58d30f5096d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='afe46653-c65b-44e7-8dee-b35a7d0b5898', node_type='1', metadata={}, hash='ca8db6161d2135b18b5438b8cef6b1b766d21678630069a6fd9480992b3a0d7e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"This too seems to be an inborn trait in humans.One of the startups we've funded, Octopart, is currently locked in\\na classic battle of good versus evil.  They're a search site for\\nindustrial components.  A lot of people need to search for components,\\nand before Octopart there was no good way to do it.  That, it turned\\nout, was no coincidence.Octopart built the right way to search for components.  Users like\\nit and they've been growing rapidly.  And yet for most of Octopart's\\nlife, the biggest distributor, Digi-Key, has been trying to force\\nthem take their prices off the site.  Octopart is sending them\\ncustomers for free, and yet Digi-Key is trying to make that traffic\\nstop.  Why?  Because their current business model depends on\\novercharging people who have incomplete information about prices.\\nThey don't want search to work.The Octoparts are the nicest guys in the world.  They dropped out\\nof the PhD program in physics at Berkeley to do this.  They just\\nwanted to fix a problem they encountered in their research.  Imagine\\nhow much time you could save the world's engineers if they could\\ndo searches online.  So when I hear that a big, evil company is\\ntrying to stop them in order to keep search broken, it makes me\\nreally want to help them. It makes me spend more time on the Octoparts\\nthan I do with most of the other startups we've funded.  It just\\nmade me spend several minutes telling you how great they are.  Why?\\nBecause they're good guys and they're trying to help the world.If you're benevolent, people will rally around you: investors,\\ncustomers, other companies, and potential employees.  In the long\\nterm the most important may be the potential employees.  I think\\neveryone knows now that \\ngood hackers are much better than mediocre\\nones.  If you can attract the best hackers to work for you, as\\nGoogle has, you have a big advantage.  And the very best hackers\\ntend to be idealistic.  They're not desperate for a job.  They can\\nwork wherever they want.\", mimetype='text/plain', start_char_idx=9990, end_char_idx=11965, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.24639511156987443)], metadata={'b97fc4ec-a34c-4d6f-86be-e3ec5c25a94f': {'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}, '8aea0f59-eece-4b2d-9b61-3dc04cc9e3da': {'file_path': '/Users/myhome/Downloads/2.LlamaIndex Tutorials/data/be-good.txt', 'file_name': 'be-good.txt', 'file_type': 'text/plain', 'file_size': 16710, 'creation_date': '2025-01-19', 'last_modified_date': '2025-01-19'}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"summary of article?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4595d62a-8344-47d2-a8d8-497527e36550",
   "metadata": {},
   "source": [
    "#### Use a different response mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b06acdd-9226-4bf7-a6c8-ccfc5610b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e364965-27ae-42bc-aae9-b36413b06d19",
   "metadata": {},
   "source": [
    "#### Stream the response back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "152a965d-b30f-4659-b007-eb5837a035ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author defines \"good\" as a strategic approach that serves as a compass for decision-making. It involves prioritizing the best interests of users, leading to exponential growth and success. Being \"good\" is highlighted as a valuable strategy in complex situations due to its stateless nature, akin to telling the truth. The concept of \"good\" is not presented in a sanctimonious manner but rather as a practical guide for making choices, forming strategies, and even designing software."
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(streaming=True)\n",
    "response = query_engine.query(\"In less than 100 words, what is the meaning of good according to the author?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76bf40-439f-4907-9a57-7521b45c6fad",
   "metadata": {},
   "source": [
    "#### Use a chatbot instead of a QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7988e811-1591-4032-a2cc-de6b0c5bba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author views being bad as deviating from the traditional notion of goodness, which is often linked to being quiet. They question the concept of goodness and recall being labeled as bad in their youth, indicating that being bad involves not conforming to conventional standards of good behavior.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_chat_engine()\n",
    "response = query_engine.chat(\"In less than 100 words, what is the meaning of bad according to the author?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7cca10d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author reflects on their childhood perception of being bad and how it contrasted with the idea of being good, which was linked to being quiet. They express skepticism towards the concept of goodness and note that their reputation is not primarily associated with being good, but rather with meaning well.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.chat(\"Oh interesting, tell me more.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc467a4-0acd-42a6-92c6-fa7b242064db",
   "metadata": {},
   "source": [
    "## Documentation: The LlamaIndex Video Series\n",
    "* Build a document chatbot from scratch\n",
    "* Sub-questions\n",
    "* Manage documents from a source that is constantly updating like Discord\n",
    "* Combining SQL and Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592ef2c-c39c-4963-9bfd-2a1be9be2c04",
   "metadata": {},
   "source": [
    "## Documentation: Use Cases\n",
    "* QA\n",
    "* Chatbots\n",
    "* Agents\n",
    "* Structured Data Extraction\n",
    "* Multi-modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de60ff2-e07b-4f49-8f72-033171d6d511",
   "metadata": {},
   "source": [
    "## Documentation: Understanding (LI vs LC)\n",
    "* Using LLMs\n",
    "    * Different way of loading OpenAIEmbeddings than LC\n",
    "    * Similar approach to Prompt templates \n",
    "* Loading\n",
    "    * Very interesting: multi-purpose loader\n",
    "    * Splitter, chunk_size, chunk_overlap\n",
    "    * Creating chunks (nodes) manually\n",
    "    * Adding metadata to document (copied to nodes)\n",
    "    * Loading connectors from LLamaHub\n",
    "* Indexing\n",
    "    * Index types:\n",
    "        * Vector store index\n",
    "            * Nodes and embeddings\n",
    "            * Semantic search\n",
    "            * Top K Retrieval\n",
    "        * Summary index\n",
    "            * If you want to summarize the document \n",
    "        * Knowledge graph index\n",
    "            * If your data is a set of disconnected concepts (a \"graph\") \n",
    "* Storing\n",
    "    * by default, indexed data is stored only in memory\n",
    "    * creating embeddings is expensive\n",
    "    * store to avoid the time and cost of re-indexing\n",
    "    * save: .persist()\n",
    "    * load persisted index: load_index_from_storage()\n",
    "* Querying\n",
    "    * the most significant part of an LLM App\n",
    "    * stages: retrieval, postprocessing, response synthesis.\n",
    "    * customizing the stages of querying.\n",
    "* Putting it all together\n",
    "    * advanced techniques\n",
    "    * how to build a full-stack app\n",
    "        * React + Flask API\n",
    "* Observability: tracing and debugging.\n",
    "    * Logging\n",
    "    * Callbacks to help debug\n",
    "    * One-click observability with eval tools offered by partners (W&B, etc)\n",
    "* Evaluation.\n",
    "    * Response evaluation\n",
    "    * Retrieval evaluation\n",
    "    * Analizing the cost of your app\n",
    "        * MockLLM to predict token usage\n",
    "        * MockEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88432aad-0140-4075-897f-184bd7d86730",
   "metadata": {},
   "source": [
    "## Documentation: Optimizing\n",
    "* Advanced prompt techniques\n",
    "* Prompt engineering for RAG\n",
    "* Advanced retrieval strategies\n",
    "* Agentic strategies\n",
    "    * OpenAI Agent\n",
    "* Evaluation\n",
    "* Fine-tuning\n",
    "* Building performant RAG apps for production\n",
    "    * General techniques\n",
    "        * decoupling retrieval chunks vs syntesis chunks\n",
    "        * structured retrieval for large document sets\n",
    "        * dynamically retrieve chunks\n",
    "        * optimize context embeddings\n",
    "    * Long list of specific techniques\n",
    "* Building RAG from scratch (lower-level)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f0185-ab10-4752-b9e0-aea066d1e354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a6bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6f6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
